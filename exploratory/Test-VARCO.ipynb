{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13af5dd5",
   "metadata": {},
   "source": [
    "## VARCO-VISION 14B\n",
    "\n",
    "* From NCSOFT\n",
    "* Ranked #35 in OpenCompass multimodal academic leaderboard (https://rank.opencompass.org.cn/leaderboard-multimodal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2ff3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import LlavaOnevisionForConditionalGeneration, AutoProcessor\n",
    "\n",
    "# Add the desired directory to the Python path\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('/data/students/earl/llava-dissector/VARCO-VISION-14B-HF'))\n",
    "\n",
    "import urllib\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "from typing import Optional, Union, List, Tuple\n",
    "\n",
    "# Sub-class LlavaOnevisionForConditionalGeneration here\n",
    "class CustomLlavaOnevisionForConditionalGeneration(LlavaOnevisionForConditionalGeneration):\n",
    "    def __init__(self, config, threshold=-1.5):\n",
    "        super().__init__(config)\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        pixel_values: torch.FloatTensor = None,\n",
    "        image_sizes: Optional[torch.LongTensor] = None,\n",
    "        pixel_values_videos: torch.FloatTensor = None,\n",
    "        image_sizes_videos: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        vision_feature_layer: Optional[int] = None,\n",
    "        vision_feature_select_strategy: Optional[str] = None,\n",
    "        vision_aspect_ratio: Optional[str] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        num_logits_to_keep: int = 0,\n",
    "    ):\n",
    "        # Call the parent class's forward method\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            pixel_values=pixel_values,\n",
    "            image_sizes=image_sizes,\n",
    "            pixel_values_videos=pixel_values_videos,\n",
    "            image_sizes_videos=image_sizes_videos,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            vision_feature_layer=vision_feature_layer,\n",
    "            vision_feature_select_strategy=vision_feature_select_strategy,\n",
    "            vision_aspect_ratio=vision_aspect_ratio,\n",
    "            labels=labels,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "            num_logits_to_keep=num_logits_to_keep\n",
    "        )    \n",
    "\n",
    "        # Modify the hidden_states before logits are computed\n",
    "        hidden_states = outputs.hidden_states[-1]  # Get the last hidden state\n",
    "        modified_hidden_states = self.modify_hidden_states(hidden_states)\n",
    "\n",
    "        # Recompute logits using the modified hidden states\n",
    "        # The key thing here is realizing that language_model hides the lm_head as its attribute\n",
    "        logits = self.language_model.lm_head(modified_hidden_states)\n",
    "\n",
    "        # Return the modified outputs\n",
    "        outputs.logits = logits\n",
    "        return outputs\n",
    "\n",
    "    def modify_hidden_states(self, hidden_states):\n",
    "        if self.threshold is not None:\n",
    "            threshold = self.threshold\n",
    "            hidden_states = torch.relu(hidden_states - threshold) + threshold\n",
    "        return hidden_states\n",
    "\n",
    "    \n",
    "model_name = \"/data/students/earl/llava-dissector/VARCO-VISION-14B-HF\"\n",
    "model = CustomLlavaOnevisionForConditionalGeneration.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=\"float16\",\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=\"flash_attention_2\"\n",
    "    )\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "device = \"cuda:5\" #model.device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42805282",
   "metadata": {},
   "source": [
    "## Extract the object and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a1bad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a chat history and use `apply_chat_template` to get correctly formatted prompt\n",
    "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\")\n",
    "cls = \"baseball player\"\n",
    "model.threshold = -10\n",
    "\n",
    "\n",
    "url = \"https://farm3.staticflickr.com/2402/2480652763_e6b62303ee_z.jpg\"\n",
    "img = urllib.request.urlopen(url=url, timeout=5).read()\n",
    "img = Image.open(BytesIO(img)).convert(\"RGB\")\n",
    "text = f'Give the normalized bounding box coordinates in the format [x1, y1, x2, y2] of all instances of {cls} in the image.'\n",
    "#text = f'Bounding box coordinates of all instances of {cls} in the image. Do not include coordinates for any other objects or text in the output. Do not output bounding boxes for all other objects.'\n",
    "\n",
    "conversation = [\n",
    "    #{\n",
    "    #    \"role\": \"system\",\n",
    "    #    \"content\": [\n",
    "    #        {\"type\": \"text\", \"text\": \"You are a helpful assistant that extracts bounding box coordinates of objects in images.\"},\n",
    "    #        {\"type\": \"text\", \"text\": \"You will be given an image and a class name, and you should output the bounding box coordinates of instances of that class in the image.\"},\n",
    "    #        {\"type\": \"text\", \"text\": \"The output should be in the format: <bbox> x1, y1, x2, y2 </bbox> for each bounding box.\"},\n",
    "    #    ],\n",
    "    {    \n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            #{\"type\": \"text\", \"text\": f\"<gro>\\nBounding box coordinates of instances of {cls} in the image. Do not include coordinates for any other objects or text in the output. Do not output bounding boxes for all other objects.\"},\n",
    "            {\"type\": \"text\", \"text\": {text}},\n",
    "            {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "EOS_TOKEN = \"<|im_end|>\"\n",
    "#image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "#raw_image = Image.open(requests.get(image_file, stream=True).raw)\n",
    "inputs = processor(images=img, text=prompt, return_tensors='pt').to(device, torch.float16)\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=1024, do_sample=False)\n",
    "output = processor.decode(output[0][inputs.input_ids.shape[1]:])\n",
    "if output.endswith(EOS_TOKEN):\n",
    "    output = output[: -len(EOS_TOKEN)]\n",
    "\n",
    "output = output.strip()\n",
    "print(output)\n",
    "\n",
    "## Visualize using cv2\n",
    "\n",
    "# Extract the bounding box coordinates from the output\n",
    "# Make sure the list is a list of floats\n",
    "# Example output: \"Bounding box coordinates: [[x1, y1, x2, y2], [x1, y1, x2, y2]]\"\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#pattern = r'<bbox>\\s*([\\d.]+)\\s*,\\s*([\\d.]+)\\s*,\\s*([\\d.]+)\\s*,\\s*([\\d.]+)\\s*</bbox>'\n",
    "pattern = r'(?:[\\[\\(]\\s*([\\d.]+)\\s*,\\s*([\\d.]+)\\s*,\\s*([\\d.]+)\\s*,\\s*([\\d.]+)\\s*[\\]\\)])|(?:<box>\\s*([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s*</box>)'\n",
    "matches = re.findall(pattern, output)\n",
    "bounding_boxes = [[float(coord) for coord in match] for match in matches]\n",
    "print(\"Bounding boxes:\", bounding_boxes)\n",
    "\n",
    "# Visualize using cv2\n",
    "import cv2\n",
    "import numpy as np\n",
    "img_cv = np.array(img)\n",
    "for box in bounding_boxes:\n",
    "    x1, y1, x2, y2 = box\n",
    "    # convert these normalized coordinates to pixel values\n",
    "    h, w, _ = img_cv.shape\n",
    "    x1 = int(x1 * w)\n",
    "    y1 = int(y1 * h)\n",
    "    x2 = int(x2 * w)\n",
    "    y2 = int(y2 * h)\n",
    "    # Draw the bounding box and label on the image\n",
    "    cv2.rectangle(img_cv, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    cv2.putText(img_cv, cls, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(img_cv)\n",
    "plt.axis('off')\n",
    "plt.show() \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
