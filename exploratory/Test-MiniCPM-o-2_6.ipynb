{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b70cbab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/students/earl/anaconda3/envs/rca/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/data/students/earl/anaconda3/envs/rca/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:524: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.57it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = '/data/students/earl/llava-dissector/MiniCPM-o-2_6'\n",
    "\n",
    "# load omni model default, the default init_vision/init_audio/init_tts is True\n",
    "# if load vision-only model, please set init_audio=False and init_tts=False\n",
    "# if load audio-only model, please set init_vision=False\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation='sdpa', # sdpa or flash_attention_2\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    init_vision=True,\n",
    "    init_audio=False,\n",
    "    init_tts=False\n",
    ")\n",
    "\n",
    "\n",
    "model = model.eval().cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, \n",
    "                                          trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d860be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/students/earl/anaconda3/envs/rca/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InterpolationMode\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GenerationConfig\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mreset_session()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbaseball bat\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mthreshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;66;03m#-1.1\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "model.reset_session()\n",
    "cls = \"baseball bat\"\n",
    "model.threshold = None #-1.1\n",
    "\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=10, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    # Check if image_file is a URL\n",
    "    if isinstance(image_file, str) and (image_file.startswith('http://') or image_file.startswith('https://')):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(io.BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "\n",
    "url = \"https://farm3.staticflickr.com/2402/2480652763_e6b62303ee_z.jpg\"\n",
    "pixel_values = load_image(url).to(torch.bfloat16)\n",
    "generation_config = GenerationConfig(max_new_tokens=1024, \n",
    "                                     temperature=0.0,\n",
    "                                     do_sample=False,\n",
    "                                     top_p=1.0,\n",
    "                                     top_k=1)\n",
    "\n",
    "text = f'Give the bounding box coordinates in the normalized format [x1, y1, x2, y2] with x1, y1 corresponding to the top left corner and x2,y2 to the bottom right corner enclosed in square brackets for each {cls} in the image, separated by newlines, and nothing else.'\n",
    "question = f'<image></image> {text}.'\n",
    "msgs = [{\"role\": \"user\", \"content\": question}]\n",
    "response = model.chat(\n",
    "    pixel_values,\n",
    "    msgs,\n",
    "    tokenizer=tokenizer,\n",
    "    generation_config=generation_config\n",
    ")\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "# regex the part of string to extract bounding boxes\n",
    "pattern = r'(?:[\\[\\(]\\s*([\\d.]+)\\s*,\\s*([\\d.]+)\\s*,\\s*([\\d.]+)\\s*,\\s*([\\d.]+)\\s*[\\]\\)])|(?:<box>\\s*([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s*</box>)'\n",
    "matches = re.findall(pattern, response)\n",
    "bounding_boxes = []\n",
    "for match in matches:\n",
    "    # Each match is a tuple of 8 elements, only 4 will be filled\n",
    "    coords = [c for c in match if c != '']\n",
    "    if len(coords) == 4:\n",
    "        bounding_boxes.append([float(c) for c in coords])\n",
    "print(f'Bounding Boxes: {bounding_boxes}')\n",
    "\n",
    "# Visualize using cv2\n",
    "# Load the original image for visualization\n",
    "image_from_url = requests.get(url)\n",
    "orig_img = Image.open(io.BytesIO(image_from_url.content)).convert('RGB')\n",
    "img_cv = np.array(orig_img)  # HWC, RGB\n",
    "for box in bounding_boxes:\n",
    "    x1, y1, x2, y2 = box\n",
    "    # convert these normalized coordinates to pixel values\n",
    "    h, w, _ = img_cv.shape\n",
    "    x1 = int(x1 * w)\n",
    "    y1 = int(y1 * h)\n",
    "    x2 = int(x2 * w)\n",
    "    y2 = int(y2 * h)\n",
    "    # Draw the bounding box and label on the image\n",
    "    cv2.rectangle(img_cv, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    cv2.putText(img_cv, cls, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(img_cv)\n",
    "plt.axis('off')\n",
    "plt.show() \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
