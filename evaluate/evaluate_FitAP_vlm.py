"""
(c)2025, earl-juanico
conda env: rca

This will be used to calculate the FitAP
from the answers generated by the VLM model (answers)
and the ground truths generated by generate_questions

It can be improved by calculating the weighted average based 
on the number of ground truth bboxes per class

The last part of this script calculates AP in accordance 
to the specifications of the COCO challenge
https://cocodataset.org/#detection-eval

Requires 

"""

import os
import json
import numpy as np
from collections import defaultdict, Counter
import h5py
import ast
import matplotlib.pyplot as plt 
import sys

sys.path.append('/data/students/earl/llava-dissector/')
from eval_coco import get_predictions_from_llava, draw_bboxes
from calculate_ap_multiple import PR, calculate_ap, calc_iou


# ablation: ['','_ablation00',...,'_ablation05',..., '_ablation10', '_kosmos5','_kosmos2', '_minigptv2','_minigpt5']
ablation = ''
split = '_novel'  # split: ['', '_novel', '_base']

DIR = '/data/students/earl/llava-dissector/'
questions_file = f"{DIR}/questions/rca_qset_val{split}.jsonl"

# Set loader/model/rca here
LOADER="qwen"  # for qwen models
#LOADER="deepseek"  # for deepseek models
#LOADER="varco"  # for varco models
#LOADER="ovis"  # for ovis models
#LOADER="valley"  # for valley models
#LOADER="sail"  # for sail models
#LOADER="ristretto"  # for sail models
#LOADER="paligemma"  # for sail models
#LOADER="moondream" # for moondream models
#LOADER="gemma"  # for gemma3 models
#LOADER="points" # for points models
#LOADER="minicpm"  # for MiniCPM models
#LOADER="kimi"  # for Kimi models

## MODEL NAMES (based on HuggingFace, as of June 25, 2025)
##
# Qwen2.5-VL-7B-Instruct, WeThink-Qwen2.5VL-7B, POINTS-1-5-Qwen-2-5-7B-Chat
# VARCO-VISION-14B-HF
# Ovis2-34B, Valley-Eagle-7B, Valley2-DPO, SAIL-VL-1d6-8B 
# DeepSeek-VL2/deepseek_vl2
# Ristretto-3B, Kimi-VL-A3B-Instruct, MiniCPM-o-2_6
# paligemma2-3b-mix-448
# moondream/moondream/torch, Gemma3-27b/gemma-3-27b-it

MODEL = "WeThink-Qwen2.5VL-7B"
RCA = "rca"  # "rca" or "none" or "rca_0" or "none_0"

answers_folder = f"answers_{LOADER}_loader"
answers_file = f"{DIR}/{answers_folder}/{MODEL}_{RCA}.jsonl"

# COCO and output paths
H5DIR = f'{DIR}/dump/'
MODEL_NAME = MODEL
prompt_mapping = {
    '': 'f',
    '_ablation00': '0',
    '_ablation01': '1',
    '_ablation02': '2',
    '_ablation03': '3',
    '_ablation04': '4',
    '_ablation05': '5',
    '_ablation06': '6',
    '_ablation07': '7',
    '_ablation08': '8',
    '_ablation09': '9',    
}
H5PY = f'{H5DIR}h5-{MODEL_NAME}{split}-p{prompt_mapping.get(ablation,"_")}.h5'

# For deepseek 
#H5PY = f'{H5DIR}h5-DeepseekVL2-{split}-p{prompt_mapping.get(ablation,"_")}.h5'
# For moondream 
#H5PY = f'{H5DIR}h5-Moondream2-{split}-p{prompt_mapping.get(ablation,"_")}.h5'
# For gemma
#H5PY = f'{H5DIR}h5-Gemma3-{split}-p{prompt_mapping.get(ablation,"_")}.h5'
 

ROOT_DIR = DIR
TRUTHS = f"llava_gt_val2017{split}.jsonl"
COCO_DIR = '/data/vlm/playground/'
coco_data_json = COCO_DIR + 'data/coco/annotations/instances_val2017.json'
image_dir = COCO_DIR + 'data/coco/val2017/'
PRC_DIR = ROOT_DIR + f'prc/prc-{MODEL_NAME}{split}{ablation}_{RCA}/'
if not os.path.exists(PRC_DIR):
    os.makedirs(PRC_DIR)

# Load the COCO dataset annotations file
with open(coco_data_json, 'r') as f:
    coco_data = json.load(f)
objects = coco_data['annotations']
images = coco_data['images']
cats = coco_data['categories']

# This is where the image names are located
questions = []
image_name = []
index = []
with open(questions_file, 'r') as f:
    for line in f:
        q = json.loads(line)
        questions.append(q)
        image_name.append(q.get('image', ''))
        index.append(q.get('question_id', ''))

coco_gt_file = ROOT_DIR + 'gt/' + TRUTHS

# Get detections and ground truths
grounds = []
cls = []
with open(coco_gt_file, 'r') as f:
    for line in f:
        gt = json.loads(line)
        grounds.append(gt.get('answers', ''))
        cls.append(gt.get('class', ''))

# Build a mapping from question_id to image filename for fast lookup
qid_to_image = {q.get('question_id'): q.get('image') for q in questions}

def normalize_bbox(bbox, w, h):
    """Normalize bbox if not in [0,1] range and image size is known."""
    if w is None or h is None:
        raise ValueError(f"Cannot normalize bbox {bbox} without image size.")
    # Normalize if any value is outside [0, 1]
    if any(v < 0 or v > 1 for v in bbox):
        x_min = bbox[0] / w
        y_min = bbox[1] / h
        x_max = bbox[2] / w
        y_max = bbox[3] / h
        normed = [x_min, y_min, x_max, y_max]
        # Debug: check if normalization worked
        if any(v > 1.0 or v < 0.0 for v in normed):
            print(f"Warning: Normalized bbox still out of range: {normed} (orig: {bbox}, w: {w}, h: {h})")
        return normed
    else:  # Added on Jun 21, 2025
        # If already normalized, return as is
        bbox = [float(v) for v in bbox]
        if len(bbox) != 4:
            raise ValueError(f"Invalid bbox format: {bbox}. Expected 4 values.")
    return bbox

predictions = []
with open(answers_file, 'r') as f:
    for line in f:
        pred = json.loads(line)
        bboxes = pred.get('bounding_boxes', [])
        qid = pred.get('question_id')
        img_name = qid_to_image.get(qid, '')
        img_path = os.path.join(image_dir, img_name)
        # Try to get image size if needed
        w, h = None, None
        if bboxes and img_name:
            try:
                from PIL import Image
                with Image.open(img_path) as img:
                    w, h = img.size
            except Exception as e:
                print(f"Error opening image {img_path}: {e}")
                w, h = None, None
        # Normalize each bbox robustly
        norm_bboxes = []
        for bbox in bboxes:
            try:
                norm_bbox = normalize_bbox(bbox, w, h)
            except Exception as e:
                print(f"Failed to normalize bbox {bbox} for image {img_path}: {e}")
                continue  # Skip this bbox if normalization fails
            # Debug: print if still not normalized
            if any(v > 1.0 for v in norm_bbox):
                print(f"Unnormalized bbox detected: {norm_bbox} (orig: {bbox}, w: {w}, h: {h}, img: {img_path})")
            norm_bboxes.append(norm_bbox)
        predictions.append([
            (bbox, round((bbox[2] - bbox[0]) * (bbox[3] - bbox[1]), 4))
            for bbox in norm_bboxes
        ])

# Then we need to accumulate the detections and ground truths for each image
detections = defaultdict(list)
truths = defaultdict(list)
for k, (im, idx, c, g, p) in enumerate(zip(image_name, index, cls, grounds, predictions)):
    detections[(im, str(idx))].append([(c, f'{bbox[0]}', round(bbox[1], 2)) for bbox in p])
    truths[(im, str(idx))].append([(c, f'{bbox}') for bbox in g])

iou_th_values = np.round(np.arange(0.5, 1.0, 0.05), 2)
classes = [cls['name'] for cls in cats]
print(f'How many categories in total: {len(classes)}')

# 1.) Get the total number of ground truth objects for every class in the dataset
total_grounds_per_class = Counter({c: 0 for c in classes})
for ((image, idx), dets), ((image, idx), grnds) in zip(detections.items(), truths.items()):
    dt_bboxes = dets[0]
    gt_bboxes = grnds[0]
    gt_counts = Counter([g[0] for g in gt_bboxes])
    for c in gt_counts:
        total_grounds_per_class[c] += gt_counts[c]

# Redefine classes to exclude the empty classes
classes = [c for c in classes if total_grounds_per_class[c] > 0]
print(f'How many categories with ground truths: {len(classes)}')

pr = defaultdict(list)
with h5py.File(H5PY, 'a') as hf:
    if 'precision' in hf:
        del hf['precision']
        print(f"Dataset 'precision' deleted")
    if 'recall' in hf:
        del hf['recall']
        print(f"Dataset 'recall' deleted")
    if 'truths' in hf:
        del hf['truths']
        print(f"Dataset 'truths' deleted")
    if 'ious' in hf:
        del hf['ious']
        print(f"Dataset 'ious' deleted")
    precision_group = hf.create_group('precision')
    recall_group = hf.create_group('recall')
    truths_group = hf.create_group('truths')
    ious_group = hf.create_group('ious')
    for iou_th in iou_th_values:
        for ((image, idx), dets), ((image, idx), grnds) in zip(detections.items(), truths.items()):
            dt_bboxes = dets[0]
            gt_bboxes = grnds[0]
            L = np.zeros((len(gt_bboxes), len(dt_bboxes)))
            N = np.zeros((len(gt_bboxes), len(dt_bboxes)))
            C = np.zeros((len(gt_bboxes), len(dt_bboxes)))
            Q = np.zeros((len(gt_bboxes), len(dt_bboxes)))
            import random
            for i, g in enumerate(gt_bboxes):
                for j, d in enumerate(dt_bboxes):
                    L[i, j] = calc_iou(ast.literal_eval(g[1]), ast.literal_eval(d[1]))
                    N[i, j] = 1 if g[0] == d[0] else -1
                    C[i, j] = d[2] * L[i, j]
                    if L[i, j] >= iou_th:
                        if int(N[i, j]) == -1:
                            Q[i, j] = -L[i, j]
                        elif int(N[i, j]) == 1:
                            Q[i, j] = L[i, j]
                    else:
                        if L[i, j] != 0:
                            Q[i, j] = -1 - L[i, j]
            if Q.size > 0:
                Q_row_min = np.max(-Q, axis=1, keepdims=True)
                Q = np.where(Q < -1, np.where(Q == -Q_row_min, -1, -2), Q)
                Q_row_max = np.max(np.where(Q == 0, -3, Q), axis=1, keepdims=True)
                temp = np.where(Q == Q_row_max, Q, -4)
                Q = np.where(Q == temp, Q, -4)
                Q_col_max = np.max(np.where(Q == -4, -5, Q), axis=0)
                temp = np.where(Q == Q_col_max, Q, 0)
                Q = np.where(Q == temp, Q, 0)
            Q_row_sum = np.sum(Q, axis=1)
            nonzero_indices = [np.nonzero(row)[0] for row in Q]
            for i, g in enumerate(gt_bboxes):
                for j in nonzero_indices[i]:
                    if Q_row_sum[i] > 0:
                        pr[(g[0], round(iou_th, 2))].append((C[i, j], 1))
                    elif Q_row_sum[i] < 0:
                        pr[(g[0], round(iou_th, 2))].append((C[i, j], 0))
            dts = [dt_box[1] for dt_box in dt_bboxes]
            gts = [gt_box[1] for gt_box in gt_bboxes]
            iou = np.amax(L, axis=0)
            cls_name = [dt_box[0] for dt_box in dt_bboxes]
            TP = np.count_nonzero(Q_row_sum > 0)
            FP = np.count_nonzero(Q_row_sum < 0)
            if iou_th == 0.5:
                image_path = image_dir + image
                image_out_path = PRC_DIR + image
                # DEBUG: 
                print(f"image_path: {image_path}, cls: {cls_name}, gts: {gts}, dts: {dts}")
                
                draw_bboxes(image_path, cls_name, gts, dts, TP, FP, iou, image_out_path)
            s = f"{cls_name[0]} in Image: {image}" if cls_name else f"Image: {image}"
            print('\n' + '*' * (len(s) + 1))
            print(s)
            print('*' * (len(s) + 1))
            formatted_L = "L = \n" + "\n".join([" ".join([f"{x:.2f}" for x in row]) for row in L])
            formatted_N = "\nN = \n" + "\n".join([" ".join([f"{x:.2f}" for x in row]) for row in N])
            formatted_C = "\nC = \n" + "\n".join([" ".join([f"{x:.4f}" for x in row]) for row in C])
            formatted_Q = "\nQ = \n" + "\n".join([" ".join([f"{x:.2f}" for x in row]) for row in Q])
            print(formatted_L)
            print(formatted_N)
            print(formatted_C)
            print(formatted_Q)
            if iou_th == 0.5:
                dataset_name = idx
                print(dataset_name)
                precision = TP / (TP + FP) if (TP + FP) > 0 else 0
                recall = TP / len(gt_bboxes) if len(gt_bboxes) > 0 else 0
                ntruths = len(gt_bboxes)
                non_zero_elements = L[Q != 0]
                q_average = np.mean(non_zero_elements) if non_zero_elements.size > 0 else 0
                if dataset_name in precision_group:
                    print(f'[Warning] Dataset {dataset_name} already exists in the HD5F file')
                else:
                    precision_group.create_dataset(dataset_name, data=precision)
                    recall_group.create_dataset(dataset_name, data=recall)
                    truths_group.create_dataset(dataset_name, data=ntruths)
                    ious_group.create_dataset(dataset_name, data=q_average)

AP = []
for iou_th in iou_th_values:
    iou_th = round(iou_th, 2)
    aP = []
    for c in classes:
        key = (c, iou_th)
        sorted_list = sorted(pr[key], key=lambda x: x[0], reverse=True)
        PrecisionRecall = defaultdict(list)
        for k, _ in enumerate(sorted_list):
            TP = sum([p[1] for p in sorted_list[:k + 1]])
            G = total_grounds_per_class[key[0]]
            precision = TP / (k + 1)
            recall = TP / G
            PrecisionRecall[key].append((precision, recall))
        points = PrecisionRecall[key]
        ap, Curve = calculate_ap(points)
        aP.append(ap)
        plt.text(0.95, 0.95, f'$\Theta$ = {iou_th:.2f}\n AP = {ap:.2f}',
                 horizontalalignment='right', verticalalignment='top',
                 transform=plt.gca().transAxes, fontsize=16, fontweight='bold')
        Curve.savefig(f'{PRC_DIR}AP_{c}__iou_{iou_th:.2f}.png', format='png')
        plt.close()
    print(f'AP@{iou_th}: {np.mean(aP)}')
    AP.append(aP)
s = f'AP: {np.mean(AP)}'
print('*' * len(s))
print(f'AP: {np.mean(AP)}')
print('*' * len(s))
